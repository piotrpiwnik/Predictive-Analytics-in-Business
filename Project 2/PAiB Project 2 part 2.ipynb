{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling - choosing the best model based on the validation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# Functions defined below are also available as utils in .py script in the `final_submissions` directory\n",
    "# Additional definitions of the assessment function included for clarity of the final submission\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to assess an instance of the fitted model on the cross-validation scorer:\n",
    "def model_cross_val_assess(model, X_train, y_train, model_name):\n",
    "    '''\n",
    "    Takes *fitted* model in args1 and applies it to the train set (predictors matrix in args2 and respone vector in args3) while also taking model's name as arg4. \n",
    "    Returns set of metrics in a dataframe object.\n",
    "\n",
    "    ### Examplary call of the function:\n",
    "    model_cross_val_assess(model, X_train, y_train, X_test, y_test, 'my_model')\n",
    "\n",
    "\n",
    "    ### How to concatenate results of several models for comparison:\n",
    "    a = model_cross_val_assess(model_1, X_train, y_train, 'my_model_1')\n",
    "    b = model_cross_val_assess(model_2, X_train, y_train, 'my_model_2')\n",
    "    c = pd.concat([a, b], axis=1)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import locale\n",
    "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    def get_num_columns(X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return len(X.columns)\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            return X.shape[1]\n",
    "        else:\n",
    "            raise TypeError('X must be a pandas DataFrame or a NumPy array')\n",
    "\n",
    "    scores_dict = {}\n",
    "\n",
    "    # Use cross-validation to evaluate the model on the training data\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_score = -scores.mean()\n",
    "    std_score = scores.std()\n",
    "    coef_variation = std_score / mean_score\n",
    "\n",
    "    ## Additionally metrics for assessment on train set\n",
    "    # R2 score\n",
    "    y_pred = model.predict(X_train)\n",
    "    r2 = r2_score(y_train, y_pred)\n",
    "\n",
    "    # Adjusted R2\n",
    "    p = get_num_columns(X_train)\n",
    "    n = len(y_train)\n",
    "    adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "    # Now return the metrics as the dict\n",
    "    scores_dict[model_name] = [r2, adj_r2, mean_score, std_score, coef_variation]\n",
    "\n",
    "    df_train_eval = pd.DataFrame(data=scores_dict, index=['r2', 'adj_r2', 'mean_cv_mse', 'std_cv_mse', 'coef_of_var'])\n",
    "    \n",
    "    # Set float_format to display numbers without scientific notation and with thousand separators\n",
    "    locale.setlocale(locale.LC_ALL, '')  # Set the locale to the user's default locale\n",
    "    pd.options.display.float_format = lambda x: format(locale.atof(f\"{x:.1f}\"), ',') if abs(x) > 999 else f\"{x:.4f}\"\n",
    "    # in case the thousand separator is displaying incorrectly, please comment out/delete the above 2 lines and use uncomment this line:\n",
    "    # pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "    \n",
    "    \n",
    "    return df_train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to assess an instance of the fitted model on the test set:\n",
    "def model_assess(model, X_test, y_test, model_name):\n",
    "    '''\n",
    "    Takes *fitted* model in args1 and applies it to the test set (predictors matrix in args2 and respone vector in args3) while also taking model's name as arg4. \n",
    "    Returns set of metrics in a dataframe object.\n",
    "\n",
    "    ### Examplary call of the function:\n",
    "    model_assess(model, X_test, y_test, 'my_model')\n",
    "\n",
    "    \n",
    "    ### How to concatenate results of several models for comparison:\n",
    "    a = model_assess(model_1, X_test, y_test, 'my_model_1')\n",
    "    b = model_assess(model_2, X_test, y_test, 'my_model_2')\n",
    "    c = pd.concat([a, b], axis=1)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import locale\n",
    "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "    def get_num_columns(X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return len(X.columns)\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            return X.shape[1]\n",
    "        else:\n",
    "            raise TypeError('X must be a pandas DataFrame or a NumPy array')\n",
    "\n",
    "    scores_dict = {}\n",
    "\n",
    "    # Access predictions of the fitted model on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    ## Metrics\n",
    "    # R2 score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Adjusted R2\n",
    "    p = get_num_columns(X_test)\n",
    "    n = len(y_test)\n",
    "    adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "    # MSE\n",
    "    mse = mean_squared_error(y_test, y_pred, squared=True)\n",
    "\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    # Now return the metrics as the dict\n",
    "    scores_dict[model_name] = [r2, adj_r2, mse, mae, mape]\n",
    "\n",
    "    df_eval = pd.DataFrame(data=scores_dict, index=['r2', 'adj_r2', 'mse', 'mae', 'mape'])\n",
    "    \n",
    "    # Set float_format to display numbers without scientific notation and with thousand separators\n",
    "    locale.setlocale(locale.LC_ALL, '')  # Set the locale to the user's default locale\n",
    "    pd.options.display.float_format = lambda x: format(locale.atof(f\"{x:.1f}\"), ',') if abs(x) > 999 else f\"{x:.4f}\"\n",
    "    # in case the thousand separator is displaying incorrectly, please comment out/delete the above 2 lines and use uncomment this line:\n",
    "    # pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "    \n",
    "    \n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System communication\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry_convert as pc\n",
    "\n",
    "# Model assessment - custom utils for systematic scoring and comparison\n",
    "# from model_cross_val_assessment import model_cross_val_assess\n",
    "# from model_assessment import model_assess\n",
    "\n",
    "# Model assessment - for at-hand scoring\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import GroupShuffleSplit, GridSearchCV, KFold, GroupKFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from scipy.stats import f_oneway, chi2_contingency, randint\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLEASE CHANGE THE WORKING/FILE DIRECTORY! THANK YOU\n",
    "#Set the working directory\n",
    "os.chdir('C:\\\\Users\\\\piotr\\\\OneDrive - Erasmus University Rotterdam\\\\Documents\\\\Github repositories\\\\Predictive-Analytics-in-Business\\\\Project 2\\\\')\n",
    "\n",
    "\n",
    "## Load the data set\n",
    "\n",
    "# Data set that was first cleaned then merged\n",
    "df_final = pd.read_csv('df_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['permalink', 'name', 'homepage_url', 'category_list', 'state_code',\n",
       "       'founded_at', 'first_funding_at', 'last_funding_at',\n",
       "       'funding_round_permalink', 'funded_at', 'acquirer_permalink',\n",
       "       'acquirer_name', 'acquirer_category_list', 'acquirer_market',\n",
       "       'acquirer_country_code', 'acquirer_state_code', 'acquirer_region',\n",
       "       'acquirer_city', 'acquired_at', 'acquired_month', 'acquired_quarter',\n",
       "       'acquired_year', 'investor_permalink', 'investor_name',\n",
       "       'funding_total_usd', 'market', 'status', 'country_code', 'region',\n",
       "       'city', 'funding_rounds', 'funding_round_type', 'funding_round_code',\n",
       "       'funded_month', 'funded_quarter', 'funded_year', 'raised_amount_usd',\n",
       "       'price_amount', 'price_currency_code', 'investor_country_code',\n",
       "       'investor_region', 'investor_city'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-int-to-str dtype for date-related features\n",
    "df_final['funded_month'] = df_final['funded_month'].astype('int64').astype('str')\n",
    "df_final['funded_quarter'] = df_final['funded_quarter'].astype('int64').astype('str')\n",
    "df_final['funded_year'] = df_final['funded_year'].astype('int64').astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small cleaning for ID/timestamp type-of columns and columns with NaN\n",
    "df_final_linear = df_final.drop(columns=['homepage_url', 'category_list', 'name', 'investor_permalink', 'price_amount',\n",
    "                       'acquirer_permalink', 'acquired_at', 'acquired_month', 'acquired_quarter', 'acquired_year', \n",
    "                       'founded_at', 'first_funding_at', 'last_funding_at','funding_round_permalink', 'funded_at'])\n",
    "\n",
    "df_final_linear.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Move target to the second column position for convenience\n",
    "df_final_linear.insert(1, 'funding_total_usd', df_final_linear.pop('funding_total_usd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further cleaning - customizable; to explore tilts in covariates\n",
    "df_final_linear = df_final_linear.drop(columns=['price_currency_code', 'acquirer_state_code', 'state_code'])\n",
    "df_final_linear.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(test_size=0.20, n_splits=1, random_state = 123)\n",
    "split = splitter.split(df_final_linear, groups=df_final_linear['permalink'])\n",
    "train_idx, test_idx = next(split)\n",
    "\n",
    "train = df_final_linear.iloc[train_idx]\n",
    "test = df_final_linear.iloc[test_idx]\n",
    "\n",
    "X_train, y_train = train.iloc[:, train.columns.get_loc('acquirer_name'):], train.iloc[:, train.columns.get_loc('funding_total_usd')]\n",
    "X_test, y_test = test.iloc[:, test.columns.get_loc('acquirer_name'):], test.iloc[:, test.columns.get_loc('funding_total_usd')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variables are stored in a list called continuous_vars\n",
    "# Categorical variables are stored in a list called categorical_vars\n",
    "\n",
    "# Get the column names of the continuous and categorical variables\n",
    "continuous_vars = list(X_train.select_dtypes(include=['int64', 'float64']).columns)\n",
    "categorical_vars = list(X_train.select_dtypes(include=['object', 'datetime64[ns]']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps for continuous variables\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing steps for categorical variables\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine noww\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('continuous', continuous_transformer, continuous_vars),\n",
    "        ('categorical', categorical_transformer, categorical_vars)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;continuous&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;funding_rounds&#x27;,\n",
       "                                                   &#x27;raised_amount_usd&#x27;]),\n",
       "                                                 (&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;acquirer_name&#x27;,\n",
       "                                                   &#x27;acquirer_category_list&#x27;,\n",
       "                                                   &#x27;acquirer_market&#x27;,\n",
       "                                                   &#x27;acquirer_country_code&#x27;,\n",
       "                                                   &#x27;acquirer_region&#x27;,\n",
       "                                                   &#x27;acquirer_city&#x27;,\n",
       "                                                   &#x27;investor_name&#x27;, &#x27;market&#x27;,\n",
       "                                                   &#x27;status&#x27;, &#x27;country_code&#x27;,\n",
       "                                                   &#x27;region&#x27;, &#x27;city&#x27;,\n",
       "                                                   &#x27;funding_round_type&#x27;,\n",
       "                                                   &#x27;funding_round_code&#x27;,\n",
       "                                                   &#x27;funded_month&#x27;,\n",
       "                                                   &#x27;funded_quarter&#x27;,\n",
       "                                                   &#x27;funded_year&#x27;,\n",
       "                                                   &#x27;investor_country_code&#x27;,\n",
       "                                                   &#x27;investor_region&#x27;,\n",
       "                                                   &#x27;investor_city&#x27;])])),\n",
       "                (&#x27;regressor&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;continuous&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;funding_rounds&#x27;,\n",
       "                                                   &#x27;raised_amount_usd&#x27;]),\n",
       "                                                 (&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;acquirer_name&#x27;,\n",
       "                                                   &#x27;acquirer_category_list&#x27;,\n",
       "                                                   &#x27;acquirer_market&#x27;,\n",
       "                                                   &#x27;acquirer_country_code&#x27;,\n",
       "                                                   &#x27;acquirer_region&#x27;,\n",
       "                                                   &#x27;acquirer_city&#x27;,\n",
       "                                                   &#x27;investor_name&#x27;, &#x27;market&#x27;,\n",
       "                                                   &#x27;status&#x27;, &#x27;country_code&#x27;,\n",
       "                                                   &#x27;region&#x27;, &#x27;city&#x27;,\n",
       "                                                   &#x27;funding_round_type&#x27;,\n",
       "                                                   &#x27;funding_round_code&#x27;,\n",
       "                                                   &#x27;funded_month&#x27;,\n",
       "                                                   &#x27;funded_quarter&#x27;,\n",
       "                                                   &#x27;funded_year&#x27;,\n",
       "                                                   &#x27;investor_country_code&#x27;,\n",
       "                                                   &#x27;investor_region&#x27;,\n",
       "                                                   &#x27;investor_city&#x27;])])),\n",
       "                (&#x27;regressor&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;continuous&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;funding_rounds&#x27;, &#x27;raised_amount_usd&#x27;]),\n",
       "                                (&#x27;categorical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;acquirer_name&#x27;, &#x27;acquirer_category_list&#x27;,\n",
       "                                  &#x27;acquirer_market&#x27;, &#x27;acquirer_country_code&#x27;,\n",
       "                                  &#x27;acquirer_region&#x27;, &#x27;acquirer_city&#x27;,\n",
       "                                  &#x27;investor_name&#x27;, &#x27;market&#x27;, &#x27;status&#x27;,\n",
       "                                  &#x27;country_code&#x27;, &#x27;region&#x27;, &#x27;city&#x27;,\n",
       "                                  &#x27;funding_round_type&#x27;, &#x27;funding_round_code&#x27;,\n",
       "                                  &#x27;funded_month&#x27;, &#x27;funded_quarter&#x27;,\n",
       "                                  &#x27;funded_year&#x27;, &#x27;investor_country_code&#x27;,\n",
       "                                  &#x27;investor_region&#x27;, &#x27;investor_city&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">continuous</label><div class=\"sk-toggleable__content\"><pre>[&#x27;funding_rounds&#x27;, &#x27;raised_amount_usd&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;acquirer_name&#x27;, &#x27;acquirer_category_list&#x27;, &#x27;acquirer_market&#x27;, &#x27;acquirer_country_code&#x27;, &#x27;acquirer_region&#x27;, &#x27;acquirer_city&#x27;, &#x27;investor_name&#x27;, &#x27;market&#x27;, &#x27;status&#x27;, &#x27;country_code&#x27;, &#x27;region&#x27;, &#x27;city&#x27;, &#x27;funding_round_type&#x27;, &#x27;funding_round_code&#x27;, &#x27;funded_month&#x27;, &#x27;funded_quarter&#x27;, &#x27;funded_year&#x27;, &#x27;investor_country_code&#x27;, &#x27;investor_region&#x27;, &#x27;investor_city&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('continuous',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['funding_rounds',\n",
       "                                                   'raised_amount_usd']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['acquirer_name',\n",
       "                                                   'acquirer_category_list',\n",
       "                                                   'acquirer_market',\n",
       "                                                   'acquirer_country_code',\n",
       "                                                   'acquirer_region',\n",
       "                                                   'acquirer_city',\n",
       "                                                   'investor_name', 'market',\n",
       "                                                   'status', 'country_code',\n",
       "                                                   'region', 'city',\n",
       "                                                   'funding_round_type',\n",
       "                                                   'funding_round_code',\n",
       "                                                   'funded_month',\n",
       "                                                   'funded_quarter',\n",
       "                                                   'funded_year',\n",
       "                                                   'investor_country_code',\n",
       "                                                   'investor_region',\n",
       "                                                   'investor_city'])])),\n",
       "                ('regressor', LinearRegression())])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit linear regression model with preprocessor\n",
    "model_linreg_f = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "model_linreg_f.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_linreg_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.7368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_r2</th>\n",
       "      <td>0.7367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_cv_mse</th>\n",
       "      <td>1.2640687314040402e+16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_cv_mse</th>\n",
       "      <td>6,866,967,301,275,708.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coef_of_var</th>\n",
       "      <td>0.5432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_linreg_f\n",
       "r2                           0.7368\n",
       "adj_r2                       0.7367\n",
       "mean_cv_mse  1.2640687314040402e+16\n",
       "std_cv_mse  6,866,967,301,275,708.0\n",
       "coef_of_var                  0.5432"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation scores on train sets\n",
    "lin_a_cv = model_cross_val_assess(model_linreg_f, X_train, y_train, 'model_linreg_f')\n",
    "# ---------\n",
    "lin_a_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most influential scores, both sides:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>investor_name_Sprint Nextel</td>\n",
       "      <td>1,161,365,053.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5049</th>\n",
       "      <td>investor_name_EquityZen</td>\n",
       "      <td>914,489,804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13982</th>\n",
       "      <td>city_The Hague</td>\n",
       "      <td>688,609,036.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4911</th>\n",
       "      <td>investor_name_Eagle River Holdings</td>\n",
       "      <td>651,900,057.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5130</th>\n",
       "      <td>investor_name_Farallon Capital Management</td>\n",
       "      <td>641,817,105.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>investor_name_Technology Venture Partners US</td>\n",
       "      <td>530,760,472.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>acquirer_name_Sprint Nextel</td>\n",
       "      <td>521,356,085.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>acquirer_city_Overland Park</td>\n",
       "      <td>521,356,085.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8624</th>\n",
       "      <td>investor_name_Tamra-Tacoma Capital Partners</td>\n",
       "      <td>519,601,226.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15761</th>\n",
       "      <td>investor_city_Overland Park</td>\n",
       "      <td>505,335,043.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>acquirer_name_Sunrise Group</td>\n",
       "      <td>500,871,339.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11709</th>\n",
       "      <td>city_Chaoyang</td>\n",
       "      <td>471,099,250.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5182</th>\n",
       "      <td>investor_name_First Solar</td>\n",
       "      <td>433,373,090.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14351</th>\n",
       "      <td>funding_round_code_H</td>\n",
       "      <td>432,949,498.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>investor_name_Chengwei Capital</td>\n",
       "      <td>427,639,677.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4064</th>\n",
       "      <td>investor_name_BrightHouse</td>\n",
       "      <td>417,824,134.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>investor_name_21st Century Fox</td>\n",
       "      <td>415,033,306.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5853</th>\n",
       "      <td>investor_name_Iconiq Capital</td>\n",
       "      <td>404,473,819.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>acquirer_region_Kansas City</td>\n",
       "      <td>375,472,823.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12998</th>\n",
       "      <td>city_Modi'in</td>\n",
       "      <td>374,810,266.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4808</th>\n",
       "      <td>investor_name_Digital Sky Technologies</td>\n",
       "      <td>372,214,173.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>investor_name_Compass Technology Partners</td>\n",
       "      <td>366,145,744.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>investor_name_Valor Equity Partners</td>\n",
       "      <td>364,011,545.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9360</th>\n",
       "      <td>investor_name_Y Ventures</td>\n",
       "      <td>347,691,263.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>investor_name_Aabar Investments</td>\n",
       "      <td>341,183,368.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>investor_name_Venture TDF</td>\n",
       "      <td>336,692,190.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6894</th>\n",
       "      <td>investor_name_Myriad</td>\n",
       "      <td>335,065,382.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8494</th>\n",
       "      <td>investor_name_Suncor Energy</td>\n",
       "      <td>325,979,838.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9633</th>\n",
       "      <td>market_Communities</td>\n",
       "      <td>321,775,349.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>investor_name_Clarium Capital</td>\n",
       "      <td>319,700,975.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383</th>\n",
       "      <td>investor_name_Legendary Entertainment</td>\n",
       "      <td>-168,602,083.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>investor_name_Avigo Capital Partners</td>\n",
       "      <td>-171,049,511.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>investor_name_THE WORLD BANK GROUP</td>\n",
       "      <td>-176,849,855.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14298</th>\n",
       "      <td>city_York</td>\n",
       "      <td>-177,772,844.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>acquirer_name_Ebix</td>\n",
       "      <td>-179,689,977.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>city_Spruce Grove</td>\n",
       "      <td>-183,979,036.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11082</th>\n",
       "      <td>region_Spruce Grove</td>\n",
       "      <td>-183,979,036.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>investor_name_Palaash Ventures</td>\n",
       "      <td>-185,032,016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13980</th>\n",
       "      <td>city_Texas</td>\n",
       "      <td>-189,361,280.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13344</th>\n",
       "      <td>city_Peoria Heights</td>\n",
       "      <td>-189,408,266.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11918</th>\n",
       "      <td>city_Delft</td>\n",
       "      <td>-190,077,551.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12721</th>\n",
       "      <td>city_Leiden</td>\n",
       "      <td>-197,176,295.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8357</th>\n",
       "      <td>investor_name_Spektra Capital, LLC.</td>\n",
       "      <td>-199,749,889.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4535</th>\n",
       "      <td>investor_name_Coolhouse Labs</td>\n",
       "      <td>-202,370,911.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>acquirer_name_Leap Wireless</td>\n",
       "      <td>-216,155,635.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12562</th>\n",
       "      <td>city_Kansas City</td>\n",
       "      <td>-224,102,232.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7663</th>\n",
       "      <td>investor_name_Quasar Ventures</td>\n",
       "      <td>-231,615,022.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8900</th>\n",
       "      <td>investor_name_Triangle Growth Partners</td>\n",
       "      <td>-246,603,811.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7141</th>\n",
       "      <td>investor_name_Norseman Capital</td>\n",
       "      <td>-250,788,636.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5742</th>\n",
       "      <td>investor_name_Hony Capital</td>\n",
       "      <td>-252,793,546.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13071</th>\n",
       "      <td>city_Murphy</td>\n",
       "      <td>-260,604,091.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>city_Bolingbrook</td>\n",
       "      <td>-273,040,080.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5831</th>\n",
       "      <td>investor_name_IQ One</td>\n",
       "      <td>-287,753,457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9222</th>\n",
       "      <td>investor_name_WUTIF</td>\n",
       "      <td>-288,130,445.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310</th>\n",
       "      <td>investor_name_LFE Capital</td>\n",
       "      <td>-304,575,882.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13743</th>\n",
       "      <td>city_Sevenoaks</td>\n",
       "      <td>-326,505,021.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12343</th>\n",
       "      <td>city_Haninge</td>\n",
       "      <td>-329,774,484.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11323</th>\n",
       "      <td>city_Aschheim</td>\n",
       "      <td>-402,534,485.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9362</th>\n",
       "      <td>investor_name_YF Capital</td>\n",
       "      <td>-528,726,755.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>investor_name_Index Capital</td>\n",
       "      <td>-656,030,010.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Feature      Importance\n",
       "8379                    investor_name_Sprint Nextel 1,161,365,053.8\n",
       "5049                        investor_name_EquityZen   914,489,804.0\n",
       "13982                                city_The Hague   688,609,036.6\n",
       "4911             investor_name_Eagle River Holdings   651,900,057.6\n",
       "5130      investor_name_Farallon Capital Management   641,817,105.9\n",
       "8671   investor_name_Technology Venture Partners US   530,760,472.7\n",
       "1113                    acquirer_name_Sprint Nextel   521,356,085.3\n",
       "2977                    acquirer_city_Overland Park   521,356,085.3\n",
       "8624    investor_name_Tamra-Tacoma Capital Partners   519,601,226.8\n",
       "15761                   investor_city_Overland Park   505,335,043.8\n",
       "1139                    acquirer_name_Sunrise Group   500,871,339.3\n",
       "11709                                 city_Chaoyang   471,099,250.6\n",
       "5182                      investor_name_First Solar   433,373,090.1\n",
       "14351                          funding_round_code_H   432,949,498.2\n",
       "4371                 investor_name_Chengwei Capital   427,639,677.9\n",
       "4064                      investor_name_BrightHouse   417,824,134.3\n",
       "3140                 investor_name_21st Century Fox   415,033,306.6\n",
       "5853                   investor_name_Iconiq Capital   404,473,819.3\n",
       "2562                    acquirer_region_Kansas City   375,472,823.7\n",
       "12998                                  city_Modi'in   374,810,266.2\n",
       "4808         investor_name_Digital Sky Technologies   372,214,173.6\n",
       "4498      investor_name_Compass Technology Partners   366,145,744.9\n",
       "9061            investor_name_Valor Equity Partners   364,011,545.5\n",
       "9360                       investor_name_Y Ventures   347,691,263.1\n",
       "3263                investor_name_Aabar Investments   341,183,368.2\n",
       "9107                      investor_name_Venture TDF   336,692,190.9\n",
       "6894                           investor_name_Myriad   335,065,382.8\n",
       "8494                    investor_name_Suncor Energy   325,979,838.2\n",
       "9633                             market_Communities   321,775,349.8\n",
       "4434                  investor_name_Clarium Capital   319,700,975.1\n",
       "6383          investor_name_Legendary Entertainment  -168,602,083.4\n",
       "3712           investor_name_Avigo Capital Partners  -171,049,511.2\n",
       "8571             investor_name_THE WORLD BANK GROUP  -176,849,855.2\n",
       "14298                                     city_York  -177,772,844.9\n",
       "392                              acquirer_name_Ebix  -179,689,977.2\n",
       "13848                             city_Spruce Grove  -183,979,036.5\n",
       "11082                           region_Spruce Grove  -183,979,036.5\n",
       "7386                 investor_name_Palaash Ventures  -185,032,016.0\n",
       "13980                                    city_Texas  -189,361,280.1\n",
       "13344                           city_Peoria Heights  -189,408,266.2\n",
       "11918                                    city_Delft  -190,077,551.1\n",
       "12721                                   city_Leiden  -197,176,295.1\n",
       "8357            investor_name_Spektra Capital, LLC.  -199,749,889.9\n",
       "4535                   investor_name_Coolhouse Labs  -202,370,911.4\n",
       "658                     acquirer_name_Leap Wireless  -216,155,635.1\n",
       "12562                              city_Kansas City  -224,102,232.2\n",
       "7663                  investor_name_Quasar Ventures  -231,615,022.7\n",
       "8900         investor_name_Triangle Growth Partners  -246,603,811.9\n",
       "7141                 investor_name_Norseman Capital  -250,788,636.7\n",
       "5742                     investor_name_Hony Capital  -252,793,546.7\n",
       "13071                                   city_Murphy  -260,604,091.6\n",
       "11499                              city_Bolingbrook  -273,040,080.6\n",
       "5831                           investor_name_IQ One  -287,753,457.0\n",
       "9222                            investor_name_WUTIF  -288,130,445.2\n",
       "6310                      investor_name_LFE Capital  -304,575,882.4\n",
       "13743                                city_Sevenoaks  -326,505,021.2\n",
       "12343                                  city_Haninge  -329,774,484.7\n",
       "11323                                 city_Aschheim  -402,534,485.8\n",
       "9362                       investor_name_YF Capital  -528,726,755.7\n",
       "5903                    investor_name_Index Capital  -656,030,010.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature names for the one-hot encoded variables\n",
    "onehot_columns = preprocessor.named_transformers_[\n",
    "    'categorical'].named_steps['onehot'].get_feature_names_out(categorical_vars)\n",
    "\n",
    "# Combine the continuous variable names and one-hot encoded variable names\n",
    "feature_names = continuous_vars + list(onehot_columns)\n",
    "\n",
    "# Get the coefficients and feature importances\n",
    "coefficients = model_linreg_f.named_steps['regressor'].coef_\n",
    "importances = coefficients\n",
    "\n",
    "# Table of feature importances\n",
    "importance_df = pd.DataFrame(\n",
    "    {'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort df by importance score\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Most important features put together\n",
    "print(\"Most influential scores, both sides:\")\n",
    "a_a = importance_df.head(30)\n",
    "b_b = importance_df.tail(30)\n",
    "c_c = pd.concat([a_a, b_b], axis=0)\n",
    "# ---------\n",
    "c_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized (Ridge) regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid for Ridge regression\n",
    "ridge_param_grid = {\n",
    "    'regressor__alpha': list(np.arange(500, 5500, step=500))\n",
    "}\n",
    "\n",
    "# Create a dictionary of regressors\n",
    "regressors = {\n",
    "    'ridge': Ridge()\n",
    "}\n",
    "\n",
    "# Create a dictionary of parameter grids\n",
    "param_grids = {\n",
    "    'ridge': ridge_param_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge best parameters: {'regressor__alpha': 3500}\n",
      "ridge best training R2 score: 0.5072623640248138\n"
     ]
    }
   ],
   "source": [
    "# Create an empty dictionary to store the results\n",
    "results_regularized = {}\n",
    "\n",
    "# Loop over the regressors\n",
    "for regressor_name, regressor in regressors.items():\n",
    "\n",
    "    # Create a pipeline for each regressor\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    # Create a grid search object for the regressor\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grids[regressor_name], cv=5, n_jobs=-1)\n",
    "\n",
    "    # Fit the grid search object on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results_regularized[regressor_name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "    }\n",
    "\n",
    "    # Print the results\n",
    "    print(regressor_name, 'best parameters:',\n",
    "          results_regularized[regressor_name]['best_params'])\n",
    "    print(regressor_name, 'best training R2 score:',\n",
    "          results_regularized[regressor_name]['best_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best fitted model\n",
    "model_ridge_f = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_ridge_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.5736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_r2</th>\n",
       "      <td>0.5735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_cv_mse</th>\n",
       "      <td>9,040,281,342,187,716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_cv_mse</th>\n",
       "      <td>5,265,048,001,484,375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coef_of_var</th>\n",
       "      <td>0.5824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model_ridge_f\n",
       "r2                           0.5736\n",
       "adj_r2                       0.5735\n",
       "mean_cv_mse 9,040,281,342,187,716.0\n",
       "std_cv_mse  5,265,048,001,484,375.0\n",
       "coef_of_var                  0.5824"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation scores on train sets\n",
    "lin_b_cv = model_cross_val_assess(model_ridge_f, X_train, y_train, 'model_ridge_f')\n",
    "# ---------\n",
    "lin_b_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_linreg_f</th>\n",
       "      <th>model_ridge_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.5736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_r2</th>\n",
       "      <td>0.7367</td>\n",
       "      <td>0.5735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_cv_mse</th>\n",
       "      <td>1.2640687314040402e+16</td>\n",
       "      <td>9,040,281,342,187,716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_cv_mse</th>\n",
       "      <td>6,866,967,301,275,708.0</td>\n",
       "      <td>5,265,048,001,484,375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coef_of_var</th>\n",
       "      <td>0.5432</td>\n",
       "      <td>0.5824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_linreg_f           model_ridge_f\n",
       "r2                           0.7368                  0.5736\n",
       "adj_r2                       0.7367                  0.5735\n",
       "mean_cv_mse  1.2640687314040402e+16 9,040,281,342,187,716.0\n",
       "std_cv_mse  6,866,967,301,275,708.0 5,265,048,001,484,375.0\n",
       "coef_of_var                  0.5432                  0.5824"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare performance - train sets with cross-validation scoring against MSE\n",
    "linear_performance_cross_val = pd.concat([lin_a_cv, lin_b_cv], axis=1)\n",
    "# ---------\n",
    "linear_performance_cross_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First defining the variables to discard. We discard price amount and price currency code here, because it is missing for the majority of the observations.\n",
    "to_remove = ['name','homepage_url', 'category_list', 'first_funding_at', 'last_funding_at', 'founded_at',\n",
    "             'funding_round_permalink', 'funded_at','investor_permalink', 'investor_name', 'acquirer_permalink', \n",
    "             'acquirer_name', 'acquirer_category_list','acquirer_state_code', 'acquirer_city', 'acquired_at', \n",
    "            'acquired_month', 'acquired_quarter', 'acquired_year', 'acquirer_country_code', 'acquirer_region', 'state_code',\n",
    "            'price_amount', 'price_currency_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are not including permalink in the variables to discard as we will use this variable to define our train test split and our cross validation fold\n",
    "# As such this variable will be removed after cross-validation folds are defined\n",
    "df_model = df_final.drop(to_remove, axis = 1)\n",
    "df_model.drop_duplicates(inplace = True)\n",
    "df_model.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# defining groups based on permalink for the splitting later\n",
    "groups = df_model['permalink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model.drop('funding_total_usd', axis = 1)\n",
    "y = df_model['funding_total_usd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new train-test split using groups based on permalinks - basically so companies don't appear in both train and test sets, which would cause data leakage\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=1, random_state = 123)\n",
    "train_idx, test_idx = next(splitter.split(X,y,groups))\n",
    "\n",
    "X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piotr\\AppData\\Local\\Temp\\ipykernel_31560\\1943101124.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.drop('permalink', axis = 1, inplace = True)\n",
      "C:\\Users\\piotr\\AppData\\Local\\Temp\\ipykernel_31560\\1943101124.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test.drop('permalink', axis = 1, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# Defining the groups for cross-validation later if wanted and dropping the extra variable permalink now\n",
    "groups_cv = X_train['permalink']\n",
    "X_train.drop('permalink', axis = 1, inplace = True)\n",
    "X_test.drop('permalink', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variables are stored in a list called continuous_vars\n",
    "# Categorical variables are stored in a list called categorical_vars}\n",
    "\n",
    "# Get the column names of the continuous and categorical variables\n",
    "continuous_vars = list(X_train.select_dtypes(include=['int64', 'float64']).columns)\n",
    "categorical_vars = list(X_train.select_dtypes(include=['object', 'datetime64[ns]']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps for continuous variables\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing steps for categorical variables\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine noww\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('continuous', continuous_transformer, continuous_vars),\n",
    "        ('categorical', categorical_transformer, categorical_vars)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model using a pipeline \n",
    "model_base_gb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;continuous&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;funding_rounds&#x27;,\n",
       "                                                   &#x27;raised_amount_usd&#x27;]),\n",
       "                                                 (&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;acquirer_market&#x27;, &#x27;market&#x27;,\n",
       "                                                   &#x27;status&#x27;, &#x27;country_code&#x27;,\n",
       "                                                   &#x27;region&#x27;, &#x27;city&#x27;,\n",
       "                                                   &#x27;funding_round_type&#x27;,\n",
       "                                                   &#x27;funding_round_code&#x27;,\n",
       "                                                   &#x27;funded_month&#x27;,\n",
       "                                                   &#x27;funded_quarter&#x27;,\n",
       "                                                   &#x27;funded_year&#x27;,\n",
       "                                                   &#x27;investor_country_code&#x27;,\n",
       "                                                   &#x27;investor_region&#x27;,\n",
       "                                                   &#x27;investor_city&#x27;])])),\n",
       "                (&#x27;regressor&#x27;, GradientBoostingRegressor())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;continuous&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;funding_rounds&#x27;,\n",
       "                                                   &#x27;raised_amount_usd&#x27;]),\n",
       "                                                 (&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;acquirer_market&#x27;, &#x27;market&#x27;,\n",
       "                                                   &#x27;status&#x27;, &#x27;country_code&#x27;,\n",
       "                                                   &#x27;region&#x27;, &#x27;city&#x27;,\n",
       "                                                   &#x27;funding_round_type&#x27;,\n",
       "                                                   &#x27;funding_round_code&#x27;,\n",
       "                                                   &#x27;funded_month&#x27;,\n",
       "                                                   &#x27;funded_quarter&#x27;,\n",
       "                                                   &#x27;funded_year&#x27;,\n",
       "                                                   &#x27;investor_country_code&#x27;,\n",
       "                                                   &#x27;investor_region&#x27;,\n",
       "                                                   &#x27;investor_city&#x27;])])),\n",
       "                (&#x27;regressor&#x27;, GradientBoostingRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;continuous&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;funding_rounds&#x27;, &#x27;raised_amount_usd&#x27;]),\n",
       "                                (&#x27;categorical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;acquirer_market&#x27;, &#x27;market&#x27;, &#x27;status&#x27;,\n",
       "                                  &#x27;country_code&#x27;, &#x27;region&#x27;, &#x27;city&#x27;,\n",
       "                                  &#x27;funding_round_type&#x27;, &#x27;funding_round_code&#x27;,\n",
       "                                  &#x27;funded_month&#x27;, &#x27;funded_quarter&#x27;,\n",
       "                                  &#x27;funded_year&#x27;, &#x27;investor_country_code&#x27;,\n",
       "                                  &#x27;investor_region&#x27;, &#x27;investor_city&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">continuous</label><div class=\"sk-toggleable__content\"><pre>[&#x27;funding_rounds&#x27;, &#x27;raised_amount_usd&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;acquirer_market&#x27;, &#x27;market&#x27;, &#x27;status&#x27;, &#x27;country_code&#x27;, &#x27;region&#x27;, &#x27;city&#x27;, &#x27;funding_round_type&#x27;, &#x27;funding_round_code&#x27;, &#x27;funded_month&#x27;, &#x27;funded_quarter&#x27;, &#x27;funded_year&#x27;, &#x27;investor_country_code&#x27;, &#x27;investor_region&#x27;, &#x27;investor_city&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('continuous',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['funding_rounds',\n",
       "                                                   'raised_amount_usd']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['acquirer_market', 'market',\n",
       "                                                   'status', 'country_code',\n",
       "                                                   'region', 'city',\n",
       "                                                   'funding_round_type',\n",
       "                                                   'funding_round_code',\n",
       "                                                   'funded_month',\n",
       "                                                   'funded_quarter',\n",
       "                                                   'funded_year',\n",
       "                                                   'investor_country_code',\n",
       "                                                   'investor_region',\n",
       "                                                   'investor_city'])])),\n",
       "                ('regressor', GradientBoostingRegressor())])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_gb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.8647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_r2</th>\n",
       "      <td>0.8647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_cv_mse</th>\n",
       "      <td>1.1597528781466464e+16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_cv_mse</th>\n",
       "      <td>9,681,187,333,044,910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coef_of_var</th>\n",
       "      <td>0.8348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            base_gb\n",
       "r2                           0.8647\n",
       "adj_r2                       0.8647\n",
       "mean_cv_mse  1.1597528781466464e+16\n",
       "std_cv_mse  9,681,187,333,044,910.0\n",
       "coef_of_var                  0.8348"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_gb_cv = model_cross_val_assess(model_base_gb, X_train, y_train, 'base_gb')\n",
    "# ---------\n",
    "base_gb_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a copy of data for random forest modelling\n",
    "df_final_rf = df_final.copy()\n",
    "\n",
    "# Further data transformation, aggregating the company information to one row.\n",
    "df = df_final_rf\n",
    "\n",
    "string_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# only keeping relevant columns\n",
    "df_final_rf = df_final_rf[['funding_total_usd', 'market', 'status', 'country_code',\n",
    "       'state_code', 'city', 'funding_rounds', 'funding_round_type',\n",
    "       'funding_round_code', 'funded_month', 'funded_quarter', 'funded_year',\n",
    "       'raised_amount_usd', 'investor_country_code',\n",
    "       'investor_city']]\n",
    "\n",
    "# obtaininng dummies from the aggregated dataset. \n",
    "# the columns in which the data appears in comma-separated lists dummies are necessary to get the most information from the set.\n",
    "dummies1 = df_final_rf['funding_round_type'].str.get_dummies(sep=',').add_prefix('funding_r_type_')\n",
    "dummies2 = df_final_rf['funding_round_code'].str.get_dummies(sep=',').add_prefix('funding_r_code_')\n",
    "dummies3 = df_final_rf['investor_city'].str.get_dummies(sep=',').add_prefix('inv_city_')\n",
    "dummies4 = df_final_rf['investor_country_code'].str.get_dummies(sep=',').add_prefix('inv_country_')\n",
    "\n",
    "#creating a dataset only with dummies\n",
    "dummies = pd.concat([dummies1, dummies2, dummies3, dummies4], axis=1)\n",
    "#merging the dataset to form with all columns\n",
    "rf_data_ready = pd.concat([df_final_rf, dummies], axis=1)\n",
    "#dropping redundant columns\n",
    "dummies_var = ['funding_round_type','funding_round_code', 'investor_city','investor_country_code']\n",
    "#final version of the dataset\n",
    "rf_data_ready = rf_data_ready.drop(dummies_var, axis = 1)\n",
    "\n",
    "# Assigning observations to X (independent variables) and y(dependent variable) \n",
    "X, y = rf_data_ready.iloc[:, 1:], rf_data_ready.iloc[:, 0]\n",
    "\n",
    "# Splitting the data into train and test sets. The train set constitutes of 80%, test set of 20%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n",
    "\n",
    "# Continuous variables are stored in a list called continuous_vars\n",
    "# Categorical variables are stored in a list called categorical_vars\n",
    "\n",
    "continuous_vars = list(X.select_dtypes(include=['float64']).columns)\n",
    "categorical_vars = list(X.select_dtypes(\n",
    "    include=['object', 'datetime64[ns]']).columns)\n",
    "\n",
    "other_vars = list(set(list(X.columns)) - set(continuous_vars) - set(categorical_vars)) #all the remaining variables\n",
    "\n",
    "# Define preprocessing steps for continuous variables\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing steps for categorical variables\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# We want the other variables to stay the same - no changes\n",
    "identity_transformer = Pipeline(steps=[\n",
    "    ('identity', FunctionTransformer(lambda x: x))\n",
    "])\n",
    "\n",
    "# Combine now into one preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('continuous', continuous_transformer, continuous_vars),\n",
    "        ('categorical', categorical_transformer, categorical_vars),\n",
    "        ('other', identity_transformer, other_vars)\n",
    "    ])\n",
    "\n",
    "# Fit random forest regressor with preprocessor\n",
    "model_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sparse_matrix', FunctionTransformer(lambda x: csr_matrix(x))),\n",
    "    ('regressor',  RandomForestRegressor())\n",
    "])\n",
    "\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "rf_cv = model_cross_val_assess(model_rf, X_train, y_train, 'model_rf')\n",
    "# ---------\n",
    "rf_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All models - performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_linreg_f</th>\n",
       "      <th>model_ridge_f</th>\n",
       "      <th>model_rf</th>\n",
       "      <th>base_gb</th>\n",
       "      <th>nodup_gb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.9583</td>\n",
       "      <td>0.8647</td>\n",
       "      <td>0.8647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_r2</th>\n",
       "      <td>0.7367</td>\n",
       "      <td>0.5735</td>\n",
       "      <td>0.9560</td>\n",
       "      <td>0.8647</td>\n",
       "      <td>0.8647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_cv_mse</th>\n",
       "      <td>1.2640687314040402e+16</td>\n",
       "      <td>9,040,281,342,187,716.0</td>\n",
       "      <td>1,265,256,157,962,275.2</td>\n",
       "      <td>1.1571254759926964e+16</td>\n",
       "      <td>1.1687546989399466e+16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_cv_mse</th>\n",
       "      <td>6,866,967,301,275,708.0</td>\n",
       "      <td>5,265,048,001,484,375.0</td>\n",
       "      <td>994,421,815,246,828.8</td>\n",
       "      <td>9,704,372,314,302,416.0</td>\n",
       "      <td>9,635,632,896,389,842.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coef_of_var</th>\n",
       "      <td>0.5432</td>\n",
       "      <td>0.5824</td>\n",
       "      <td>0.7859</td>\n",
       "      <td>0.8387</td>\n",
       "      <td>0.8244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_linreg_f           model_ridge_f  \\\n",
       "r2                           0.7368                  0.5736   \n",
       "adj_r2                       0.7367                  0.5735   \n",
       "mean_cv_mse  1.2640687314040402e+16 9,040,281,342,187,716.0   \n",
       "std_cv_mse  6,866,967,301,275,708.0 5,265,048,001,484,375.0   \n",
       "coef_of_var                  0.5432                  0.5824   \n",
       "\n",
       "                           model_rf                 base_gb  \\\n",
       "r2                           0.9583                  0.8647   \n",
       "adj_r2                       0.9560                  0.8647   \n",
       "mean_cv_mse 1,265,256,157,962,275.2  1.1571254759926964e+16   \n",
       "std_cv_mse    994,421,815,246,828.8 9,704,372,314,302,416.0   \n",
       "coef_of_var                  0.7859                  0.8387   \n",
       "\n",
       "                           nodup_gb  \n",
       "r2                           0.8647  \n",
       "adj_r2                       0.8647  \n",
       "mean_cv_mse  1.1687546989399466e+16  \n",
       "std_cv_mse  9,635,632,896,389,842.0  \n",
       "coef_of_var                  0.8244  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please note that the thousand separators may appear incorrectly for you - if that is the case, please modify the functions at the beginning of the file as mentioned in the comments of the functions\n",
    "all_performance_cross_val = pd.concat([linear_performance_cross_val, rf_cv, base_gb_cv], axis=1)\n",
    "# ---------\n",
    "all_performance_cross_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the gradient boosting model with the preprocessing changes performes the best out of all the models tested across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modelling - improving the chosen model based on business knowlegde of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a copy of data for random forest modelling\n",
    "df_final_rf = df_final.copy()\n",
    "\n",
    "# Transformation to obtain enriched location information\n",
    "df_final_rf['investor_country_code'] = df_final_rf['investor_country_code'].replace('ROM', 'ROU')\n",
    "\n",
    "df_final_rf['country_code'] = df_final_rf['country_code'].replace('ROM', 'ROU')\n",
    "\n",
    "# This function converts country codes alpha 3 to continent names, which might help grasp some additional variation in the data.\n",
    "def country_to_continent(country_code):\n",
    "    if country_code == 'None':\n",
    "        return 'None'\n",
    "    country_alpha2 = pc.country_alpha3_to_country_alpha2(country_code)\n",
    "    continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "    return pc.convert_continent_code_to_continent_name(continent_code)\n",
    "\n",
    "df_final_rf['investor_continent'] = df_final_rf['investor_country_code'].apply(country_to_continent)\n",
    "df_final_rf['continent'] = df_final_rf['country_code'].apply(country_to_continent)\n",
    "\n",
    "df_final_rf = df_final_rf.loc[df_final_rf['price_currency_code'].str.strip() != 'USD']\n",
    "\n",
    "# Only keeping the columns which will be used to run random forest.\n",
    "# The choice about dropping many columns was due to lack of observations for most of the companies.\n",
    "\n",
    "#['funding_total_usd', 'market', 'status', 'country_code','continent',\n",
    "#       'state_code', 'city', 'funding_rounds', 'funding_round_type',\n",
    "#       'funding_round_code', 'funded_month', 'funded_quarter', 'funded_year',\n",
    "#       'raised_amount_usd', 'investor_country_code',\n",
    "#       'investor_city', 'investor_continent']\n",
    "\n",
    "# Further data transformation, aggregating the company information to one row.\n",
    "df = df_final_rf\n",
    "\n",
    "string_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a dictionary of aggregation functions to apply to each column\n",
    "agg_dict = {}\n",
    "for col in df.columns:\n",
    "    if col in string_cols:\n",
    "         # for categorical variables, put the unique values per permalink into a list\n",
    "        agg_dict[col] = lambda x: x.unique().tolist()\n",
    "    else:\n",
    "         # for numerical variables, select the maximum raised amount variable, \n",
    "        # otherwise pick the first number (this should never happen though since we only have raised amount \n",
    "        agg_dict[col] = 'sum' if col == 'raised_amount_usd' else 'first'\n",
    "\n",
    "# Group by name column and aggregate\n",
    "df = df.groupby(\"permalink\", as_index=False).agg(agg_dict)\n",
    "\n",
    "# select the categorical/string columns and remove the square brackets that are generated by a list, then join them with a comma\n",
    "string_cols = df.select_dtypes(include=['object']).columns\n",
    "df[string_cols] = df[string_cols].apply(lambda x: x.str.join(',').str.replace(r'\\[|\\]', ''))\n",
    "\n",
    "df_final_rf = df\n",
    "\n",
    "# only keeping relevant columns\n",
    "df_final_rf = df_final_rf[['funding_total_usd', 'market', 'status', 'country_code','continent',\n",
    "       'state_code', 'city', 'funding_rounds', 'funding_round_type',\n",
    "       'funding_round_code', 'funded_month', 'funded_quarter', 'funded_year',\n",
    "       'raised_amount_usd', 'investor_country_code',\n",
    "       'investor_city', 'investor_continent']]\n",
    "\n",
    "\n",
    "# obtaininng dummies from the aggregated dataset. \n",
    "# the columns in which the data appears in comma-separated lists dummies are necessary to get the most information from the set.\n",
    "dummies1 = df_final_rf['funding_round_type'].str.get_dummies(sep=',').add_prefix('funding_r_type_')\n",
    "dummies2 = df_final_rf['funding_round_code'].str.get_dummies(sep=',').add_prefix('funding_r_code_')\n",
    "dummies3 = df_final_rf['investor_city'].str.get_dummies(sep=',').add_prefix('inv_city_')\n",
    "dummies4 = df_final_rf['investor_country_code'].str.get_dummies(sep=',').add_prefix('inv_country_')\n",
    "dummies7 = df_final_rf['investor_continent'].str.get_dummies(sep=',').add_prefix('inv_continent_')\n",
    "\n",
    "#creating a dataset only with dummies\n",
    "dummies = pd.concat([dummies1, dummies2, dummies3, dummies4, dummies7], axis=1)\n",
    "#merging the dataset to form with all columns\n",
    "rf_data_ready = pd.concat([df_final_rf, dummies], axis=1)\n",
    "#dropping redundant columns\n",
    "dummies_var = ['funding_round_type','funding_round_code','investor_continent', 'investor_city','investor_country_code']\n",
    "#final version of the dataset\n",
    "rf_data_ready = rf_data_ready.drop(dummies_var, axis = 1)\n",
    "\n",
    "# Assigning observations to X (independent variables) and y(dependent variable) \n",
    "X, y = rf_data_ready.iloc[:, 1:], rf_data_ready.iloc[:, 0]\n",
    "\n",
    "# Splitting the data into train and test sets. The train set constitutes of 80%, test set of 20%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n",
    "\n",
    "# Continuous variables are stored in a list called continuous_vars\n",
    "# Categorical variables are stored in a list called categorical_vars\n",
    "\n",
    "continuous_vars = list(X.select_dtypes(include=['float64']).columns)\n",
    "categorical_vars = list(X.select_dtypes(\n",
    "    include=['object', 'datetime64[ns]']).columns)\n",
    "\n",
    "other_vars = list(set(list(X.columns)) - set(continuous_vars) - set(categorical_vars)) #all the remaining variables\n",
    "\n",
    "# Define preprocessing steps for continuous variables\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing steps for categorical variables\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# We want the other variables to stay the same - no changes\n",
    "identity_transformer = Pipeline(steps=[\n",
    "    ('identity', FunctionTransformer(lambda x: x))\n",
    "])\n",
    "\n",
    "\n",
    "# Combine now into one preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('continuous', continuous_transformer, continuous_vars),\n",
    "        ('categorical', categorical_transformer, categorical_vars),\n",
    "        ('other', identity_transformer, other_vars)\n",
    "    ])\n",
    "\n",
    "# Fit random forest regressor with preprocessor\n",
    "model_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sparse_matrix', FunctionTransformer(lambda x: csr_matrix(x))),\n",
    "    ('regressor',  RandomForestRegressor())\n",
    "])\n",
    "\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "rf_cv = model_cross_val_assess(model_rf, X_train, y_train, 'model_rf')\n",
    "# ---------\n",
    "rf_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the validation score is lower, and supposedly model overfits less to the data. The MSE has been reduced a lot compared to the initial score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set - the best model (random forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest has the lowest MSE out of all models, and very high R2, which makes it the best candidate. However, supposdly this performance is too high as model is overfitting a little bit. In the next part we will test different loss functions in line with the business knowledge. Additionally, new constaints will be added (if suitable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_rf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.8138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_r2</th>\n",
       "      <td>0.7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse</th>\n",
       "      <td>425,375,900,138,899.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>3,346,597.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mape</th>\n",
       "      <td>7.682636327594463e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_rf\n",
       "r2                    0.8138\n",
       "adj_r2                0.7635\n",
       "mse    425,375,900,138,899.8\n",
       "mae              3,346,597.9\n",
       "mape   7.682636327594463e+17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_test = model_assess(model_rf, X_test, y_test, 'model_rf')\n",
    "# ---------\n",
    "rf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our vusiness knowledge and the insights from the data exploration  (the first notebook). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can try for the loss function?\n",
    "Pinball? 25/75%?\n",
    "L1?\n",
    "\n",
    "Some (or all) of the categorical variables are one-hot encoded / are dummies. We can try to find a good middle solution (as suggested in the lecture). - mostly done in pre-processing in general and some additional parts for random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore values of different errors on CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Scores: [3.48243835e+14 8.32950855e+14 3.20098969e+15 1.25034774e+15\n",
      " 7.63380824e+14]\n",
      "Average Mean Squared Error: 1279182587804112.0\n",
      "Mean Absolute Error Scores: [3096055.00297911 3578249.11747253 4798567.33521874 4431568.10922747\n",
      " 3805335.77392574]\n",
      "Average Mean Absolute Error: 3941955.0677647204\n",
      "Median Absolute Error Scores: [1074.46   682.635 1144.315 1006.115  905.045]\n",
      "Average Median Absolute Error: 962.5139999999344\n",
      "R² Scores: [0.87750366 0.79239215 0.64080024 0.74841245 0.72381758]\n",
      "Average R² Score: 0.7565852173675663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_alternative(model, X_train, y_train):\n",
    "    # Using Mean Squared Error (MSE)\n",
    "    scores_mse = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    print(\"Mean Squared Error Scores:\", -scores_mse)\n",
    "    print(\"Average Mean Squared Error:\", np.mean(-scores_mse))\n",
    "\n",
    "    # Using Mean Absolute Error (MAE)\n",
    "    scores_mae = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    print(\"Mean Absolute Error Scores:\", -scores_mae)\n",
    "    print(\"Average Mean Absolute Error:\", np.mean(-scores_mae))\n",
    "\n",
    "    # Using Median Absolute Error\n",
    "    scores_median = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_median_absolute_error', n_jobs=-1)\n",
    "    print(\"Median Absolute Error Scores:\", -scores_median)\n",
    "    print(\"Average Median Absolute Error:\", np.mean(-scores_median))\n",
    "\n",
    "    # Using R² Score\n",
    "    scores_r2 = cross_val_score(model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "    print(\"R² Scores:\", scores_r2)\n",
    "    print(\"Average R² Score:\", np.mean(scores_r2))\n",
    "\n",
    "    # Using Mean Squared Log Error (MSLE)\n",
    "    scores_msle = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_log_error', n_jobs=-1)\n",
    "    print(\"Mean Squared Log Error Scores:\", -scores_msle)\n",
    "    print(\"Average Mean Squared Log Error:\", np.mean(-scores_msle))\n",
    "\n",
    "    # Using Explained Variance Score\n",
    "    scores_explained_variance = cross_val_score(model, X_train, y_train, cv=5, scoring='explained_variance', n_jobs=-1)\n",
    "    print(\"Explained Variance Scores:\", scores_explained_variance)\n",
    "    print(\"Average Explained Variance Score:\", np.mean(scores_explained_variance))\n",
    "\n",
    "# Example usage:\n",
    "evaluate_model_alternative(model_rf, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the default loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the loss function just like in skgarden tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install quantile-forest\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "# reg = RandomForestQuantileRegressor()\n",
    "\n",
    "\n",
    "# Fit random forest regressor with preprocessor\n",
    "model_rf_quant = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # Removed the sparse_matrix conversion step\n",
    "    ('regressor', RandomForestQuantileRegressor())\n",
    "])\n",
    "\n",
    "reg = model_rf_quant.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\piotr\\OneDrive - Erasmus University Rotterdam\\Documents\\Github repositories\\Predictive-Analytics-in-Business\\Project 2\\PAiB Project 2 part 2.ipynb Cell 64\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y344sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(X_train)), np\u001b[39m.\u001b[39mfull(\u001b[39mlen\u001b[39m(X_test), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y344sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y344sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m reg \u001b[39m=\u001b[39m model_rf_quant\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y344sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Make predictions using the fitted model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y344sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m y_pred_mix \u001b[39m=\u001b[39m reg\u001b[39m.\u001b[39mpredict(X_mixed_df, quantiles\u001b[39m=\u001b[39m[\u001b[39m0.25\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.75\u001b[39m, \u001b[39m0.9\u001b[39m], oob_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, indices\u001b[39m=\u001b[39mindices)\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    405\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 406\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    408\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\quantile_forest\\_quantile_forest.py:103\u001b[0m, in \u001b[0;36mBaseForestQuantileRegressor.fit\u001b[1;34m(self, X, y, sample_weight, sparse_pickle)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sparse_pickle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     76\u001b[0m     \u001b[39m\"\"\"Build a forest from the training set (X, y).\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39msuper\u001b[39;49m(BaseForestQuantileRegressor, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    104\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(X, y, multi_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, dtype\u001b[39m=\u001b[39mDTYPE)\n\u001b[0;32m    106\u001b[0m     \u001b[39m# Sort the target values in ascending order.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[39m# Use sorter to maintain mapping to original order.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:474\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    463\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    465\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    466\u001b[0m ]\n\u001b[0;32m    468\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    475\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    476\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    477\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    478\u001b[0m )(\n\u001b[0;32m    479\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    480\u001b[0m         t,\n\u001b[0;32m    481\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    482\u001b[0m         X,\n\u001b[0;32m    483\u001b[0m         y,\n\u001b[0;32m    484\u001b[0m         sample_weight,\n\u001b[0;32m    485\u001b[0m         i,\n\u001b[0;32m    486\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    487\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    488\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    489\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    491\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\joblib\\parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1052\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1055\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    865\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    783\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m     \u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   1248\u001b[0m         X,\n\u001b[0;32m   1249\u001b[0m         y,\n\u001b[0;32m   1250\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1251\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1252\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pycountry_convert as pc\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "# Define your preprocessing steps and other pipeline components...\n",
    "\n",
    "# Fit random forest regressor with preprocessor\n",
    "model_rf_quant = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('to_dense', FunctionTransformer(lambda x: np.asarray(x.todense()), accept_sparse=True)),  # Convert to np.ndarray\n",
    "    ('regressor', RandomForestQuantileRegressor())\n",
    "])\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are defined...\n",
    "\n",
    "# Concatenate X_train and X_test to create X_mixed\n",
    "X_mixed = np.concatenate([X_train, X_test])\n",
    "\n",
    "# Recreate the DataFrame structure for X_mixed\n",
    "columns = X_train.columns  # Assuming X_train and X_test have the same columns\n",
    "X_mixed_df = pd.DataFrame(X_mixed, columns=columns)\n",
    "\n",
    "# Create indices array\n",
    "indices = np.concatenate([np.arange(len(X_train)), np.full(len(X_test), -1, dtype=int)])\n",
    "\n",
    "# Fit the model\n",
    "reg = model_rf_quant.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the fitted model\n",
    "y_pred_mix = reg.predict(X_mixed_df, quantiles=[0.25, 0.5, 0.75, 0.9], oob_score=True, indices=indices)\n",
    "y_pred_train_oob = y_pred_mix[:len(X_train)]  # OOB predictions on the training data\n",
    "y_pred_test = y_pred_mix[-len(X_test):]  # Predictions on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\piotr\\OneDrive - Erasmus University Rotterdam\\Documents\\Github repositories\\Predictive-Analytics-in-Business\\Project 2\\PAiB Project 2 part 2.ipynb Cell 64\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y332sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X_mixed_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X_mixed, columns\u001b[39m=\u001b[39mcolumns)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y332sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(X_train)), np\u001b[39m.\u001b[39mfull(\u001b[39mlen\u001b[39m(X_test), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y332sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y_pred_mix \u001b[39m=\u001b[39m reg\u001b[39m.\u001b[39;49mpredict(X_mixed_df, quantiles\u001b[39m=\u001b[39;49m[\u001b[39m0.25\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, \u001b[39m0.75\u001b[39;49m], oob_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, indices\u001b[39m=\u001b[39;49mindices)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y332sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m y_pred_train_oob \u001b[39m=\u001b[39m y_pred_mix[:\u001b[39mlen\u001b[39m(X_train)]  \u001b[39m# predictions on the training data are OOB\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/piotr/OneDrive%20-%20Erasmus%20University%20Rotterdam/Documents/Github%20repositories/Predictive-Analytics-in-Business/Project%202/PAiB%20Project%202%20part%202.ipynb#Y332sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m y_pred_test \u001b[39m=\u001b[39m y_pred_mix[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(X_test):]\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\pipeline.py:482\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    481\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39mtransform(Xt)\n\u001b[1;32m--> 482\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\quantile_forest\\_quantile_forest.py:480\u001b[0m, in \u001b[0;36mBaseForestQuantileRegressor.predict\u001b[1;34m(self, X, quantiles, interpolation, weighted_quantile, weighted_leaves, aggregate_leaves_first, oob_score, indices, duplicates)\u001b[0m\n\u001b[0;32m    477\u001b[0m     X_indices \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_samples_leaf \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:  \u001b[39m# optimize for single-sample-per-leaf performance\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m     leaf_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((\u001b[39mlen\u001b[39;49m(X), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators))\n\u001b[0;32m    481\u001b[0m     y_train_leaves \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforest_\u001b[39m.\u001b[39my_train_leaves)\n\u001b[0;32m    482\u001b[0m     y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforest_\u001b[39m.\u001b[39my_train)\n",
      "File \u001b[1;32mc:\\Users\\piotr\\anaconda3\\envs\\python39\\lib\\site-packages\\scipy\\sparse\\_base.py:345\u001b[0m, in \u001b[0;36mspmatrix.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 345\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse matrix length is ambiguous; use getnnz()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m or shape[0]\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assuming X_train and X_test are pandas DataFrames\n",
    "# # Recreate the DataFrame structure for X_mixed\n",
    "\n",
    "\n",
    "# X_mixed = np.concatenate([X_train, X_test])\n",
    "\n",
    "# columns = X_train.columns  # or X_test.columns, they should be the same\n",
    "# X_mixed_df = pd.DataFrame(X_mixed, columns=columns)\n",
    "\n",
    "# indices = np.concatenate([np.arange(len(X_train)), np.full(len(X_test), -1, dtype=int)])\n",
    "# y_pred_mix = reg.predict(X_mixed_df, quantiles=[0.25, 0.5, 0.75], oob_score=True, indices=indices)\n",
    "# y_pred_train_oob = y_pred_mix[:len(X_train)]  # predictions on the training data are OOB\n",
    "# y_pred_test = y_pred_mix[-len(X_test):]  # predictions on the new test data are IB\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming y_test are the actual values and y_pred_test contains the quantile predictions\n",
    "actual = y_test  # Replace with your actual test values\n",
    "predicted_25th = y_pred_test[:, 0]  # 25th percentile predictions\n",
    "predicted_50th = y_pred_test[:, 1]  # 50th percentile (median) predictions\n",
    "predicted_75th = y_pred_test[:, 2]  # 75th percentile predictions\n",
    "predicted_90th = y_pred_test[:, 3]  # 75th percentile predictions\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(actual)), actual, label='Actual Values', color='blue')\n",
    "plt.plot(predicted_25th, label='25th Percentile', color='green')\n",
    "plt.plot(predicted_50th, label='50th Percentile', color='orange')\n",
    "plt.plot(predicted_75th, label='75th Percentile', color='red')\n",
    "plt.plot(predicted_90th, label='90th Percentile', color='purple')\n",
    "plt.fill_between(range(len(actual)), predicted_25th, predicted_75th, color='gray', alpha=0.2)\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Predicted/Actual Value')\n",
    "plt.title('Quantile Regression Predictions vs Actual Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for RandomForestQuantileRegressor\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],  # Number of trees\n",
    "    'regressor__max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'regressor__min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'regressor__min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    # Additional parameters specific to RandomForestQuantileRegressor can be added here\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(model_rf_quant, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model using GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "# Use the best model for predictions or further analysis\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining the tuning with the quantile regression, a model that is not only accurate, but also aligned with investor's interests can be developed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
